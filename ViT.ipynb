{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n",
    "      super().__init__()\n",
    "      self.patcher = nn.Sequential(\n",
    "          # We use conv for doing the patching\n",
    "          nn.Conv2d(\n",
    "              in_channels=in_channels,\n",
    "              out_channels=embed_dim,\n",
    "              # if kernel_size = stride -> no overlap\n",
    "              kernel_size=patch_size,\n",
    "              stride=patch_size\n",
    "          ),\n",
    "          # Linear projection of Flattened Patches. We keep the batch and the channels (b,c,h,w)\n",
    "          nn.Flatten(2))\n",
    "      self.cls_token = nn.Parameter(torch.randn(size=(1, 1, embed_dim)), requires_grad=True)\n",
    "      self.position_embeddings = nn.Parameter(torch.randn(size=(1, num_patches+1, embed_dim)), requires_grad=True)\n",
    "      self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "      # Create a copy of the cls token for each of the elements of the BATCH\n",
    "      cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "      # Create the patches\n",
    "      x = self.patcher(x).permute(0, 2, 1)\n",
    "      # Unify the position with the patches\n",
    "      x = torch.cat([cls_token, x], dim=1)\n",
    "      # Patch + Position Embedding\n",
    "      x = self.position_embeddings + x\n",
    "      x = self.dropout(x)\n",
    "      return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
